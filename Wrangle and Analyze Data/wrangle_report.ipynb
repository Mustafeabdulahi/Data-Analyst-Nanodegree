{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "    \n",
    "   This report demonstrates the data wrangle and analysis process for WeRateDogs. My goal for this project is to demonstarate the data analysis process, work flow, and to create interesting and trustworthy analyses and visualizations. \n",
    "    In this report I will provide a breaf introduction of the data wrangle process that is used to gether, access, clean, and analyzed the these datasets.\n",
    "    \n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "   The dataset that we will be working with is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10\n",
    "    \n",
    "\n",
    "### Data Gathering\n",
    "   Gethering Data, three pieces of datasets was required in these section. Each file is gethered a differen method as follows\n",
    "\n",
    "The `Twitter_archive_enhanced.csv` is directly downloaded from the WeRateDogs Twitter archive data. this file contains basic tweet data for all 5000+ of their tweets.\n",
    "\n",
    "The `Tweet image prediction (image_predictions.tsv)` is downloaded from Udacity using python request library and the URl provided.\n",
    "\n",
    "The `Tweet_json.txt` I downloaded Tweeter API using the Tweepy library and my own twitter developer credentials.\n",
    "\n",
    "### Assess Data\n",
    "\n",
    "   After obtaining all the required datasets, I moved onto the next step of the data wrangle which is 'Assessing data'. In this step my task was to act as detective and inspect the data quality issues and lack of tidiness using python pandas library to evaluate the data visually and programatically assesments. Upon exploring the data the following quality and and data tidness were observed.\n",
    "\n",
    "\n",
    "### Quality issues:\n",
    "\n",
    "##### Inhanced twitter archived table\n",
    "\n",
    "`Enhanced twitter archive table`\n",
    "1. Delete retweets and replies and keep the original ratings, \n",
    "2. The `timestamp` The timestamp column is in incorrect datatype format \n",
    "3. The column `Name` has missing values & invalid names\n",
    "4. the `source` column has a useless html structure that need to be fixed.\n",
    "5. in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id are float, should all be str\n",
    "\n",
    "\n",
    "##### Prediction table\n",
    "\n",
    "6. p1_dog, p2_dog, & p3_dog are not all lowercase.\n",
    "7. Remove entries that have p1_dog, p2_dog, & p3_dog values set to false. they real dogs.\n",
    "7. Remove duplicate jpg_url entrie\n",
    "\n",
    "##### tweet_json table\n",
    "8. Convert data type of tweet_id to object string data type for merging\n",
    "\n",
    "##### Tidiness\n",
    "1. twitter_archive: doggo, floofer, pupper, puppo are all stages of dog, should be in one column\n",
    "2. Merge the three datasets into one Master dataset.\n",
    "\n",
    "\n",
    "### Cleaning Data\n",
    "In this section I have cleaned the quality and tidiness issues I observed above using python and pandas library and combined all the datasets into a master dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusions and limitations\n",
    "In this report I have beriefly explained the wrangling data analysis steps and analized the dataset at it is best. I have enjoyed going through the wrangling and data analysis i have learned in this course and solving the quality issues and tidiness we have observed inproject. \n",
    "\n",
    "#### Limitations \n",
    "Finding the data quality issues, tidiness and cleaning were the hardest and time-consuming part for me, but at the end i was able to overcome this struggle. after cleaning the data and storing the master dataset there was some limitation due to the number missing value we have in dataset, for instance the dog_stage column was missing over 80% tweets don't provide dog stage info in there tweets, so for this analysis may not be 100%  guarenteed to be proof error solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
